# ReAct Planner for Dropout Prevention

## 1. Overview
The ReAct Planner acts as the cognitive core of the Agentic AI Intervention Layer. Using the **Reason-Act-Reflect** framework, it autonomously analyzes signals from the Behavioral Drift System and Risk Predictor, determines *why* a student is struggling, selects the best intervention strategy, and continuously refines its approach by reflecting on prior successes and failures.

## 2. State Representation ($S_t$)
The environment state passed to the agent includes both the current telemetry and historical context.

```json
{
  "student_id": "STU_1001",
  "drift_metrics": {
    "drift_score": 0.85,
    "login_variance": 0.5,
    "session_drop": 0.4,
    "quiz_decay": 0.1,
    "hesitation_increase": 0.9
  },
  "risk_assessment": {
    "dropout_prob": 0.72,
    "predicted_dropout_days": 5
  },
  "context": {
    "current_module": "Calculus II",
    "demographics": "Part-time, Working Professional"
  },
  "memory": [
    {
      "intervention_id": "INT_452",
      "strategy_used": "Motivation Boost",
      "timestamp": "2026-02-15",
      "success_score": 0.1
    }
  ]
}
```

## 3. Action Space ($A$)
The agent selects from a discreet set of predefined strategies. Each strategy determines the downstream Generative AI parameters.

- `micro_nudge`: Brief encouraging message mapped to short, actionable steps.
- `content_simplification`: Repackaging complex material using analogies and summaries.
- `schedule_restructure`: Adjusting deadlines and suggesting a revised timetable.
- `peer_sync`: Connecting the student with a study group or forum discussion.
- `human_escalation`: Directly alerting academic advising for manual outreach.

## 4. ReAct Framework Logic

### Phase 1: Reason (Diagnosis)
The agent analyzes the `drift_metrics` and `risk_assessment` to hypothesize a root cause.

**Heuristic Mapping Rules:**
- **Burnout**: High `login_variance`, high `session_drop`, moderate `quiz_decay`.
- **Cognitive Overload**: Extreme `hesitation_increase`, high `quiz_decay`, normal `login_variance`.
- **Conceptual Confusion**: High `hesitation_increase` on specific topics, moderate `session_drop`.
- **Disengagement**: High `login_variance`, extreme `session_drop`, low `hesitation_increase` (they aren't even trying).

*Example Output:* "The student exhibits a 90% increase in hesitation time while login frequency remains stable. This indicates they are trying to study but are stuck on the material. **Root Cause: Cognitive Overload**."

### Phase 2: Act (Strategy Selection)
The agent maps the hypothesized root cause to a baseline intervention strategy.

- **Burnout** $\rightarrow$ `schedule_restructure`
- **Cognitive Overload** $\rightarrow$ `content_simplification`
- **Conceptual Confusion** $\rightarrow$ `peer_sync` or `content_simplification`
- **Disengagement** $\rightarrow$ `micro_nudge`

*Emergency Override:* If `predicted_dropout_days` $\le 2$, auto-select `human_escalation`.

### Phase 3: Reflect (Memory Integration)
Before finalizing the action, the agent queries the `memory` array (past interventions on this student).

**Reflection Rule:**
If the previously used strategy for this root cause resulted in a low `success_score` ($\le 0.5$), the agent must **pivot** to an alternative or escalated strategy.

*Example Thought Trace:* "I initially selected 'Motivation Boost' for Disengagement. However, my reflection on memory shows that we tried 'Motivation Boost' on 2026-02-15 and it yielded a low success score of 0.1. I will pivot to 'Human Escalation' instead."

## 5. Reward Definition ($R_t$)
After an intervention ($t$), we observe the student for a time delta ($\Delta t = 3$ days). A positive reward is issued if the student's trajectory stabilizes or improves.

$$ R_t = w_{drift} \max(0, D_{score, t} - D_{score, t+\Delta t}) + w_{time} (T_{drop, t+\Delta t} - T_{drop, t}) $$
*Where $D_{score}$ is the Normalized Drift Score and $T_{drop}$ is predicted days until dropout.*
If the student drops out, $R_t = -100$.

## 6. Policy Improvement Approach
The mapping from "Root Cause $\rightarrow$ Action" is not static. It is governed by a **Contextual Bandit Algorithm** (e.g., Thompson Sampling), acting as the policy network $\pi(a|s)$.

1. **State Embedding**: The `drift_metrics` and `risk_assessment` are embedded into a dense vector.
2. **Action Selection**: The bandit samples from the beta distributions associated with each strategy arm.
3. **Update Step**: Upon observing $R_t$, the success metric updates the underlying distribution for that arm, making the system globally smarter over time while the `Reflect` phase handles student-specific personalization.

## 7. JSON Example Output

The final output generated by the Agentic Planner, which is passed to the Generative AI layer and saved to system logs.

```json
{
  "trace_id": "REQ_8891_A",
  "reasoning_trace": {
    "thought": "Drift vector shows massive hesitation increase. Student is staring at Calculus material without interacting. Root Cause diagnosed as Cognitive Overload.",
    "act_proposed": "content_simplification",
    "reflect": "No previous 'content_simplification' attempts failed in memory. Proceeding with proposed action.",
    "final_action": "content_simplification"
  },
  "action_payload": {
    "strategy": "content_simplification",
    "tone": "Analytical and Supportive",
    "channel": "In-App Modal",
    "parameters": {
      "focus_topic": "Calculus II - Integration by Parts",
      "format_type": "Analogy-based"
    }
  }
}
```
